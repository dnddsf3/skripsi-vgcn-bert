{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 0) Install dependencies\n",
        "# ----------------------------\n",
        "!pip install -q transformers datasets iterative-stratification sentencepiece --upgrade\n",
        "!pip install -q torch torchvision torchaudio --upgrade\n",
        "!pip install -q scikit-learn imbalanced-learn seaborn"
      ],
      "metadata": {
        "id": "ftw89LBwtfJg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:07:16.121596Z",
          "iopub.execute_input": "2026-01-04T10:07:16.121885Z",
          "iopub.status.idle": "2026-01-04T10:09:56.859967Z",
          "shell.execute_reply.started": "2026-01-04T10:07:16.121853Z",
          "shell.execute_reply": "2026-01-04T10:09:56.858956Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1) Imports & config\n",
        "# ----------------------------\n",
        "import os, random, math, time, re, warnings, shutil, itertools, string\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from torch.cuda.amp import autocast as amp_autocast\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "B8qnTQRPtlc7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:09:56.862446Z",
          "iopub.execute_input": "2026-01-04T10:09:56.862754Z",
          "iopub.status.idle": "2026-01-04T10:10:07.171474Z",
          "shell.execute_reply.started": "2026-01-04T10:09:56.862711Z",
          "shell.execute_reply": "2026-01-04T10:10:07.170942Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2) Configuration & Grid\n",
        "# ----------------------------\n",
        "CONFIG = {\n",
        "    \"BERT_MAIN\": \"bert-base-uncased\",\n",
        "    \"BERT_FOR_NODE_EMB\": \"distilbert-base-uncased\",\n",
        "    \"VOCAB_SIZE\": 5000,\n",
        "    \"WINDOW\": 20,\n",
        "    \"MIN_COOC\": 3,\n",
        "    \"MAX_SEQ_LEN\": 200,\n",
        "    \"PATIENCE\": 3\n",
        "}\n",
        "\n",
        "GRID = {\n",
        "    \"lr\": [1e-5, 2e-5],\n",
        "    \"dropout\": [0.2],\n",
        "    \"max_len\": [200, 256],\n",
        "    \"npmi_th\": [0.2],\n",
        "    \"gcn_hidden\": [128],\n",
        "    \"gcn_layers\": [1, 2],\n",
        "    \"graph_scale\": [0.0, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "RUN = {\"max_vocab\": CONFIG[\"VOCAB_SIZE\"], \"num_folds\": 5, \"epochs\": 8, \"batch_size\": 16}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "Ye9jZLwhtpyf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:07.172281Z",
          "iopub.execute_input": "2026-01-04T10:10:07.172775Z",
          "iopub.status.idle": "2026-01-04T10:10:07.261356Z",
          "shell.execute_reply.started": "2026-01-04T10:10:07.172750Z",
          "shell.execute_reply": "2026-01-04T10:10:07.260727Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3) Cleaning\n",
        "# ----------------------------\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n",
        "def clean_for_graph(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "    text = re.sub(r\"[0-9]+\", \" \", text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in STOPWORDS]\n",
        "    words = [LEMM.lemmatize(w) for w in words]\n",
        "    text = \" \".join(words)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_for_bert(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = text.replace(\"\\\\\", \" \")\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "H04L-e0btucz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:07.262328Z",
          "iopub.execute_input": "2026-01-04T10:10:07.262562Z",
          "iopub.status.idle": "2026-01-04T10:10:07.386809Z",
          "shell.execute_reply.started": "2026-01-04T10:10:07.262540Z",
          "shell.execute_reply": "2026-01-04T10:10:07.386239Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4) Load dataset\n",
        "# ----------------------------\n",
        "def load_dataset_hf_to_df():\n",
        "    ds = load_dataset(\"jingjietan/essays-big5\")\n",
        "    df = ds[\"train\"].to_pandas()\n",
        "    required = ['text','O','C','E','A','N']\n",
        "    for c in required:\n",
        "        if c not in df.columns: raise ValueError(f\"Missing {c}\")\n",
        "    df = df[required].copy()\n",
        "    for c in ['O','C','E','A','N']:\n",
        "        df[c] = df[c].astype(float); df[c] = (df[c] >= 0.5).astype(int)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset_hf_to_df()\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "print(\"\\nDataset:\")\n",
        "display(df[[\"text\", \"O\", \"C\", \"E\", \"A\", \"N\"]].head(5))\n",
        "\n",
        "df[\"clean_text_graph\"] = df[\"text\"].apply(clean_for_graph)\n",
        "df[\"clean_text_bert\"]  = df[\"text\"].apply(clean_for_bert)\n",
        "\n",
        "print(\"\\nSample after cleaning:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n--- Dataset {i} ---\")\n",
        "    print(\"Original :\", df.loc[i, \"text\"][:200])\n",
        "    print(\"Graph    :\", df.loc[i, \"clean_text_graph\"][:200])\n",
        "    print(\"BERT     :\", df.loc[i, \"clean_text_bert\"][:200])"
      ],
      "metadata": {
        "id": "Oe7bfQujyBa_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:07.387620Z",
          "iopub.execute_input": "2026-01-04T10:10:07.387847Z",
          "iopub.status.idle": "2026-01-04T10:10:23.944539Z",
          "shell.execute_reply.started": "2026-01-04T10:10:07.387820Z",
          "shell.execute_reply": "2026-01-04T10:10:23.943913Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4) Dataset distribution\n",
        "# ----------------------------\n",
        "def plot_label_distribution(df, traits=['O','C','E','A','N']):\n",
        "    label_counts = {}\n",
        "\n",
        "    for t in traits:\n",
        "        label_counts[t] = df[t].value_counts().sort_index()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    bar_width = 0.35\n",
        "    x = range(len(traits))\n",
        "\n",
        "    zeros = [label_counts[t].get(0, 0) for t in traits]\n",
        "    ones  = [label_counts[t].get(1, 0) for t in traits]\n",
        "\n",
        "    bars_zero = ax.bar([i - bar_width/2 for i in x], zeros, width=bar_width, label='Label 0')\n",
        "    bars_one  = ax.bar([i + bar_width/2 for i in x], ones,  width=bar_width, label='Label 1')\n",
        "\n",
        "    for bar in bars_zero:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    for bar in bars_one:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(traits)\n",
        "    ax.set_ylabel(\"Total Data\")\n",
        "    ax.set_title(\"Distribution Label for Each Big Five Trait\")\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nsKJV6NlP-j3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:23.968262Z",
          "iopub.execute_input": "2026-01-04T10:10:23.968517Z",
          "iopub.status.idle": "2026-01-04T10:10:24.086420Z",
          "shell.execute_reply.started": "2026-01-04T10:10:23.968495Z",
          "shell.execute_reply": "2026-01-04T10:10:24.085646Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4) Dataset distribution FULL\n",
        "# ----------------------------\n",
        "ds = load_dataset(\"jingjietan/essays-big5\")\n",
        "\n",
        "df_full = pd.concat([\n",
        "    ds['train'].to_pandas(),\n",
        "    ds['validation'].to_pandas(),\n",
        "    ds['test'].to_pandas(),\n",
        "], ignore_index=True)\n",
        "\n",
        "plot_label_distribution(df_full)"
      ],
      "metadata": {
        "id": "rvbljH9sc9Qn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.087240Z",
          "iopub.execute_input": "2026-01-04T10:10:24.087530Z",
          "iopub.status.idle": "2026-01-04T10:10:24.855948Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.087507Z",
          "shell.execute_reply": "2026-01-04T10:10:24.855298Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4) Data validation\n",
        "# ----------------------------\n",
        "if \"text\" not in df.columns:\n",
        "    raise ValueError(\"Column 'text' is missing from the dataframe!\")\n",
        "\n",
        "invalid_text = df[\"text\"].isna() | (df[\"text\"].astype(str).str.strip() == \"\")\n",
        "if invalid_text.any():\n",
        "    print(f\"Detected {invalid_text.sum()} rows with empty text. These rows will be removed!\")\n",
        "    df = df[~invalid_text].reset_index(drop=True)\n",
        "\n",
        "traits = ['O', 'C', 'E', 'A', 'N']\n",
        "\n",
        "for t in traits:\n",
        "    if t not in df.columns:\n",
        "        raise ValueError(f\"Label column '{t}' is missing from the dataframe!\")\n",
        "    unique_vals = sorted(df[t].unique().tolist())\n",
        "\n",
        "    if not set(unique_vals).issubset({0, 1}):\n",
        "        raise ValueError(f\"Label '{t}' contains non-binary values: {unique_vals}\")\n",
        "\n",
        "print(\"Data validation completed: 'text' is valid and labels are binary.\")"
      ],
      "metadata": {
        "id": "HYvV7ajQZLaz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.856843Z",
          "iopub.execute_input": "2026-01-04T10:10:24.857118Z",
          "iopub.status.idle": "2026-01-04T10:10:24.866455Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.857096Z",
          "shell.execute_reply": "2026-01-04T10:10:24.865773Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5) Simple tokenizer for vocab build\n",
        "# ----------------------------\n",
        "word_pattern = re.compile(r\"\\w+\")\n",
        "\n",
        "def tokenize_simple(text):\n",
        "    return word_pattern.findall(text.lower())\n",
        "\n",
        "print(\"Samples after tokenization:\")\n",
        "for i in range(5):\n",
        "    txt = df[\"clean_text_graph\"].iloc[i]\n",
        "    tokens = tokenize_simple(txt)\n",
        "\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(\"Clean Graph Text :\", txt)\n",
        "    print(\"Tokens           :\", tokens)"
      ],
      "metadata": {
        "id": "FR6X7GaWtzYD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.867305Z",
          "iopub.execute_input": "2026-01-04T10:10:24.867633Z",
          "iopub.status.idle": "2026-01-04T10:10:24.883566Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.867609Z",
          "shell.execute_reply": "2026-01-04T10:10:24.882848Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6) NPMI adjacency + helpers\n",
        "# ----------------------------\n",
        "def build_vocab_and_cooc(texts, max_vocab):\n",
        "    counter = Counter()\n",
        "\n",
        "    for t in texts:\n",
        "        counter.update(tokenize_simple(t))\n",
        "    most = [w for w, _ in counter.most_common(max_vocab)]\n",
        "    vocab = {w: i for i, w in enumerate(most)}\n",
        "    return vocab, counter\n",
        "\n",
        "def build_cooccurrence_counts(texts, vocab, window=20):\n",
        "    cooc = defaultdict(int)\n",
        "    freq = defaultdict(int)\n",
        "    total_windows = 0\n",
        "\n",
        "    for doc in texts:\n",
        "        toks = [t for t in tokenize_simple(doc) if t in vocab]\n",
        "        n = len(toks)\n",
        "        for i, w in enumerate(toks):\n",
        "            freq[w] += 1\n",
        "            start = max(0, i - window)\n",
        "            end = min(n, i + window + 1)\n",
        "            for j in range(start, end):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                cooc[(w, toks[j])] += 1\n",
        "                total_windows += 1\n",
        "\n",
        "    if total_windows == 0:\n",
        "        total_windows = 1\n",
        "    return cooc, freq, total_windows\n",
        "\n",
        "def build_npmi_adjacency(\n",
        "    texts,\n",
        "    max_vocab=5000,\n",
        "    window=20,\n",
        "    min_cooc=3,\n",
        "    npmi_th=0.15\n",
        "):\n",
        "    vocab, _ = build_vocab_and_cooc(texts, max_vocab)\n",
        "\n",
        "    if len(vocab) == 0:\n",
        "        return None, vocab\n",
        "    cooc, freq, total_windows = build_cooccurrence_counts(\n",
        "        texts, vocab, window\n",
        "    )\n",
        "\n",
        "    rows, cols, vals = [], [], []\n",
        "\n",
        "    for (w, u), c in cooc.items():\n",
        "        if c < min_cooc:\n",
        "            continue\n",
        "        p_ij = c / total_windows\n",
        "        p_i = freq[w] / total_windows\n",
        "        p_j = freq[u] / total_windows\n",
        "        if p_ij <= 0 or p_i <= 0 or p_j <= 0:\n",
        "            continue\n",
        "        pmi = math.log(p_ij / (p_i * p_j) + 1e-12)\n",
        "        denom = -math.log(p_ij + 1e-12)\n",
        "        if denom <= 0:\n",
        "            continue\n",
        "        npmi = pmi / denom\n",
        "        if npmi >= npmi_th:\n",
        "            i = vocab[w]\n",
        "            j = vocab[u]\n",
        "            rows += [i, j]\n",
        "            cols += [j, i]\n",
        "            vals += [npmi, npmi]\n",
        "\n",
        "    V = len(vocab)\n",
        "\n",
        "    if len(rows) == 0:\n",
        "        A = sp.eye(V, format=\"csr\")\n",
        "    else:\n",
        "        A = sp.coo_matrix((vals, (rows, cols)), shape=(V, V)).tocsr()\n",
        "        A = A + sp.eye(V)\n",
        "        deg = np.array(A.sum(axis=1)).flatten()\n",
        "        deg_inv_sqrt = np.power(deg, -0.5)\n",
        "        deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n",
        "        D = sp.diags(deg_inv_sqrt)\n",
        "        A = D.dot(A).dot(D)\n",
        "\n",
        "    return A, vocab"
      ],
      "metadata": {
        "id": "iKZwtSqHt31U",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.884366Z",
          "iopub.execute_input": "2026-01-04T10:10:24.884989Z",
          "iopub.status.idle": "2026-01-04T10:10:24.897556Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.884966Z",
          "shell.execute_reply": "2026-01-04T10:10:24.896762Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6) Top NPMI\n",
        "# ----------------------------\n",
        "def extract_top_npmi_edges(\n",
        "    texts,\n",
        "    max_vocab=5000,\n",
        "    window=20,\n",
        "    min_cooc=3,\n",
        "    npmi_th=0.15,\n",
        "    top_k=20\n",
        "):\n",
        "    A, vocab = build_npmi_adjacency(\n",
        "        texts,\n",
        "        max_vocab=max_vocab,\n",
        "        window=window,\n",
        "        min_cooc=min_cooc,\n",
        "        npmi_th=npmi_th\n",
        "    )\n",
        "\n",
        "    if A is None or len(vocab) == 0:\n",
        "        print(\"No vocab found.\")\n",
        "        return\n",
        "\n",
        "    inv_vocab = {i: w for w, i in vocab.items()}\n",
        "    A_coo = A.tocoo()\n",
        "    edges = []\n",
        "\n",
        "    for i, j, val in zip(A_coo.row, A_coo.col, A_coo.data):\n",
        "        if i >= j:\n",
        "            continue\n",
        "        edges.append((\n",
        "            inv_vocab[i],\n",
        "            inv_vocab[j],\n",
        "            float(val)\n",
        "        ))\n",
        "\n",
        "    edges = sorted(edges, key=lambda x: x[2], reverse=True)\n",
        "    print(f\"Vocab size : {len(vocab)}\")\n",
        "    print(f\"Total edges: {len(edges)}\")\n",
        "    print(\"\\nTop NPMI edges:\")\n",
        "\n",
        "    for w1, w2, val in edges[:top_k]:\n",
        "        print(f\"('{w1}', '{w2}') → weight = {val:.4f}\")"
      ],
      "metadata": {
        "id": "B44IsH8t4bTQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.898382Z",
          "iopub.execute_input": "2026-01-04T10:10:24.898750Z",
          "iopub.status.idle": "2026-01-04T10:10:24.912319Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.898725Z",
          "shell.execute_reply": "2026-01-04T10:10:24.911645Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6) Print NPMI examples\n",
        "# ----------------------------\n",
        "texts = df[\"clean_text_graph\"].tolist()\n",
        "A, vocab = build_npmi_adjacency(texts, max_vocab=5000, window=20, min_cooc=3, npmi_th=0.15)\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Adjacency shape:\", A.shape)\n",
        "\n",
        "print(\"\\nSamples of word ID:\")\n",
        "print(list(vocab.items())[:10])\n",
        "\n",
        "print(\"\\nSamples after NPMI:\")\n",
        "extract_top_npmi_edges(\n",
        "    texts,\n",
        "    max_vocab=5000,\n",
        "    window=20,\n",
        "    min_cooc=3,\n",
        "    npmi_th=0.15,\n",
        "    top_k=10\n",
        ")"
      ],
      "metadata": {
        "id": "isi-JHEx4cw8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:10:24.913221Z",
          "iopub.execute_input": "2026-01-04T10:10:24.913483Z",
          "iopub.status.idle": "2026-01-04T10:10:47.628263Z",
          "shell.execute_reply.started": "2026-01-04T10:10:24.913462Z",
          "shell.execute_reply": "2026-01-04T10:10:47.627562Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 7) Node embeddings from BERT\n",
        "# ----------------------------\n",
        "def build_node_embeddings_from_bert(\n",
        "    vocab_words,\n",
        "    bert_model_name,\n",
        "    device=DEVICE,\n",
        "    batch_size=128,\n",
        "    shared_model=None,\n",
        "    shared_tokenizer=None\n",
        "):\n",
        "    tokenizer = shared_tokenizer or AutoTokenizer.from_pretrained(bert_model_name) # distilbert-base-uncased\n",
        "    model = shared_model or AutoModel.from_pretrained(bert_model_name).to(device) # distilbert-base-uncased\n",
        "    model.eval()\n",
        "\n",
        "    embs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(vocab_words), batch_size):\n",
        "            batch_words = vocab_words[i:i+batch_size]\n",
        "\n",
        "            enc = tokenizer(\n",
        "                batch_words,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = enc[\"input_ids\"].to(device)\n",
        "            attn_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "            out = model(input_ids=input_ids, attention_mask=attn_mask)\n",
        "            token_embs = out.last_hidden_state\n",
        "\n",
        "            mask = attn_mask.unsqueeze(-1)\n",
        "            pooled = (token_embs * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "\n",
        "            embs.append(pooled.cpu().numpy())\n",
        "\n",
        "    if len(embs) == 0:\n",
        "        return np.zeros((0, model.config.hidden_size), dtype=np.float32)\n",
        "\n",
        "    return np.vstack(embs).astype(np.float32)"
      ],
      "metadata": {
        "id": "11s6B_tFt7kj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.040482Z",
          "iopub.execute_input": "2026-01-04T10:11:11.040754Z",
          "iopub.status.idle": "2026-01-04T10:11:11.047455Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.040730Z",
          "shell.execute_reply": "2026-01-04T10:11:11.046620Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 8) Dataset class\n",
        "# ----------------------------\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=200, graph_feats=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(np.float32)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.graph_feats = graph_feats\n",
        "\n",
        "        self.graph_dim = graph_feats.shape[1] if graph_feats is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "        if self.graph_feats is not None:\n",
        "            item['graph'] = torch.tensor(self.graph_feats[idx], dtype=torch.float32)\n",
        "        else:\n",
        "            item['graph'] = torch.zeros(self.graph_dim, dtype=torch.float32) if self.graph_dim else torch.zeros((1,), dtype=torch.float32)\n",
        "\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "5NRf0BYKuAET",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.048487Z",
          "iopub.execute_input": "2026-01-04T10:11:11.049147Z",
          "iopub.status.idle": "2026-01-04T10:11:11.063450Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.049121Z",
          "shell.execute_reply": "2026-01-04T10:11:11.062715Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 9) GCN + VGCN_BERT model\n",
        "# ----------------------------\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        if n_layers==1:\n",
        "            self.layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        else:\n",
        "            self.layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "            for _ in range(n_layers-1):\n",
        "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.act = nn.ReLU()\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        with amp_autocast(enabled=False):\n",
        "            X = X.float()\n",
        "            if isinstance(A, torch.Tensor):\n",
        "                A = A.float()\n",
        "            h = X\n",
        "            for l in self.layers:\n",
        "                if isinstance(A, torch.Tensor) and A.is_sparse:\n",
        "                    h = torch.sparse.mm(A, h)\n",
        "                else:\n",
        "                    h = A @ h\n",
        "                h = l(h); h = self.act(h)\n",
        "            return h\n",
        "\n",
        "class VGCN_BERT_Model(nn.Module):\n",
        "    def __init__(self, bert_model, gcn, hidden_dim, num_classes, dropout=0.3):\n",
        "        super(VGCN_BERT_Model, self).__init__()\n",
        "\n",
        "        self.bert = bert_model\n",
        "        self.gcn = gcn\n",
        "\n",
        "        bert_hidden = bert_model.config.hidden_size\n",
        "\n",
        "        self.graph_pool = nn.Linear(gcn.out_features, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(bert_hidden + hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        graph_node_emb=None,\n",
        "        A=None,\n",
        "        graph_feat=None,\n",
        "        graph_scale=1.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        graph_scale:\n",
        "        - 0.0  : graph contribution OFF\n",
        "        - 0–1  : weighted fusion\n",
        "        - 1.0  : full graph contribution\n",
        "        \"\"\"\n",
        "\n",
        "        # BERT\n",
        "        bert_out = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        cls = bert_out.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # GRAPH (GCN)\n",
        "        if (graph_node_emb is not None) and (A is not None):\n",
        "            node_emb = graph_node_emb.to(cls.device)\n",
        "            A = A.to(cls.device)\n",
        "\n",
        "            gcn_out = self.gcn(node_emb, A) * graph_scale\n",
        "\n",
        "            pooled = gcn_out.mean(dim=0)\n",
        "            pooled = self.graph_pool(pooled)\n",
        "            pooled = pooled.unsqueeze(0).expand(cls.size(0), -1)\n",
        "\n",
        "            fused = torch.cat([cls, pooled], dim=1)\n",
        "\n",
        "        # GRAPH (NON-GCN, OPTIONAL)\n",
        "        elif graph_feat is not None:\n",
        "            graph_feat = graph_feat.to(cls.device) * graph_scale\n",
        "            fused = torch.cat([cls, graph_feat], dim=1)\n",
        "\n",
        "        # TEXT ONLY\n",
        "        else:\n",
        "            fused = torch.cat(\n",
        "                [cls, torch.zeros(cls.size(0), self.graph_pool.out_features).to(cls.device)],\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "        # CLASSIFICATION\n",
        "        logits = self.classifier(fused)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "1mszZJPvuEba",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.064499Z",
          "iopub.execute_input": "2026-01-04T10:11:11.064714Z",
          "iopub.status.idle": "2026-01-04T10:11:11.082186Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.064694Z",
          "shell.execute_reply": "2026-01-04T10:11:11.081449Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 10) helpers: sparse convert, graph feat builder, metrics\n",
        "# ----------------------------\n",
        "def sparse_scipy_to_torch_sparse(A_csr, device=DEVICE):\n",
        "# Fungsi: mengubah adjacency matrix dari format SciPy sparse (CSR/COO) menjadi PyTorch sparse tensor\n",
        "# Dipakai untuk: torch.sparse.mm(A, h) di SimpleGCN.forward()\n",
        "    A_coo = A_csr.tocoo()\n",
        "    indices = torch.LongTensor([A_coo.row, A_coo.col])\n",
        "    values = torch.FloatTensor(A_coo.data)\n",
        "    shape = A_coo.shape\n",
        "    A_torch = torch.sparse.FloatTensor(indices, values, torch.Size(shape)).to(device)\n",
        "    return A_torch\n",
        "\n",
        "def compute_sample_graph_feats(bow_matrix, node_embs_for_graph):\n",
        "# Fungsi: membangun fitur graf per dokumen (per esai) berbasis BoW + embedding node (kata)\n",
        "# Dipakai untuk: jalur graph_feat pada VGCN_BERT_Model.forward() (opsional), yaitu menggabungkan [CLS] dengan embedding dokumen dari “graph feature”\n",
        "    rows = []\n",
        "    for i in range(bow_matrix.shape[0]):\n",
        "        row = bow_matrix.getrow(i)\n",
        "        idxs = row.indices\n",
        "        if len(idxs)==0:\n",
        "            rows.append(np.zeros(node_embs_for_graph.shape[1], dtype=np.float32)); continue\n",
        "        freqs = row.data.astype(float)\n",
        "        emb = np.zeros(node_embs_for_graph.shape[1], dtype=np.float32)\n",
        "        for k, f in zip(idxs, freqs):\n",
        "            emb += node_embs_for_graph[k] * float(f)\n",
        "        emb = emb / (freqs.sum() + 1e-9)\n",
        "        norm = np.linalg.norm(emb)\n",
        "        if norm > 0: emb = emb / (norm + 1e-9)\n",
        "        rows.append(emb)\n",
        "    return np.vstack(rows).astype(np.float32)\n",
        "\n",
        "def tune_thresholds(y_true, y_probs, low=0.2, high=0.7, step=0.01):\n",
        "# Fungsi: mencari threshold terbaik per label untuk memaksimalkan F1\n",
        "# Dipakai untuk: threshold tuning OOF agar F1 meningkat dibanding threshold default 0.5\n",
        "    best = [0.5]*y_true.shape[1]\n",
        "    for i in range(y_true.shape[1]):\n",
        "        best_f1 = -1; best_t = 0.5\n",
        "        for t in np.arange(low, high, step):\n",
        "            p = (y_probs[:,i] >= t).astype(int)\n",
        "            f1 = f1_score(y_true[:,i], p, zero_division=0)\n",
        "            if f1 > best_f1: best_f1 = f1; best_t = t\n",
        "        best[i] = best_t\n",
        "    return best\n",
        "\n",
        "def compute_pos_weight(y):\n",
        "# Fungsi: menghitung pos_weight untuk BCEWithLogitsLoss agar menangani imbalance label\n",
        "# Dipakai untuk: membuat loss lebih “memperhatikan” kelas positif jika lebih jarang\n",
        "    N = y.shape[0]; pos = y.sum(axis=0); neg = N - pos\n",
        "    pw = (neg / (pos + 1e-6)); pw = np.clip(pw, 1.0, 100.0)\n",
        "    return torch.tensor(pw, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "def plot_f1_vs_thresholds(y_true, y_probs, trait_names=['O','C','E','A','N'], low=0.1, high=0.9, step=0.02):\n",
        "# Fungsi: mem-plot grafik F1 vs threshold untuk tiap trait\n",
        "# Dipakai untuk: menunjukkan bahwa threshold optimal tidak selalu 0.5 dan melihat bentuk trade-off\n",
        "    thresholds = np.arange(low, high, step)\n",
        "    n = len(trait_names)\n",
        "    plt.figure(figsize=(5*n,4))\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        scores = []\n",
        "        for t in thresholds:\n",
        "            p = (y_probs[:,i] >= t).astype(int)\n",
        "            scores.append(f1_score(y_true[:,i], p, zero_division=0))\n",
        "        ax = plt.subplot(1, n, i+1)\n",
        "        ax.plot(thresholds, scores, marker='o')\n",
        "        ax.set_title(trait)\n",
        "        ax.set_xlabel('Threshold')\n",
        "        ax.set_ylabel('F1')\n",
        "        ax.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tLQnBhlIuJDF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.083161Z",
          "iopub.execute_input": "2026-01-04T10:11:11.083438Z",
          "iopub.status.idle": "2026-01-04T10:11:11.102123Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.083413Z",
          "shell.execute_reply": "2026-01-04T10:11:11.101563Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 11) Grid search + Training (per-fold NPMI)\n",
        "# ----------------------------\n",
        "def run_grid_search_and_train(df, grid, bert_name_main, bert_name_node_emb, max_vocab, window, min_cooc, num_folds, epochs, batch_size, patience):\n",
        "    X = df['clean_text_graph'].values\n",
        "    Y = df[['O','C','E','A','N']].values\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
        "\n",
        "    tokenizer_cache = {}\n",
        "    def get_tokenizer(name, max_len):\n",
        "        key = (name, max_len)\n",
        "        if key not in tokenizer_cache:\n",
        "            tokenizer_cache[key] = AutoTokenizer.from_pretrained(name)\n",
        "        return tokenizer_cache[key]\n",
        "\n",
        "    # embedding kata (node) untuk vocabulary graph\n",
        "    node_tokenizer_shared = AutoTokenizer.from_pretrained(bert_name_node_emb)\n",
        "    node_model_shared = AutoModel.from_pretrained(bert_name_node_emb).to(DEVICE)\n",
        "    node_model_shared.eval()\n",
        "\n",
        "    # global vocab candidates (hanya untuk fallback kalau build_npmi_adjacency gagal)\n",
        "    vocab_full, _ = build_vocab_and_cooc(X, max_vocab=max_vocab)\n",
        "    vocab_words_full = sorted(vocab_full.keys(), key=lambda w: vocab_full[w])\n",
        "    print(\"Global vocab candidates:\", len(vocab_words_full))\n",
        "\n",
        "    # kombinasi konfigurasi grid hyperparameter\n",
        "    keys = list(grid.keys())\n",
        "    combos = list(itertools.product(*[grid[k] for k in keys]))\n",
        "    total_configs = len(combos)\n",
        "    results = []\n",
        "\n",
        "    graph_cache = {}\n",
        "\n",
        "    cfg_id = 0\n",
        "    for combo in combos:\n",
        "        cfg_id += 1\n",
        "        cfg = dict(zip(keys, combo))\n",
        "        lr = cfg['lr']; dropout = cfg['dropout']; max_len = cfg['max_len']; npmi_th = cfg['npmi_th']; gcn_hidden = cfg['gcn_hidden']; gcn_layers = cfg['gcn_layers']; graph_scale = cfg['graph_scale']\n",
        "        print(\"\\n\" + \"=\"*160)\n",
        "        print(f\"Running Config {cfg_id}/{total_configs}: {cfg}\")\n",
        "        print(\"=\"*160)\n",
        "        oof_preds = np.zeros_like(Y, dtype=float)\n",
        "        oof_trues = np.zeros_like(Y, dtype=int)\n",
        "        fold_scores = []\n",
        "        fold_infos = []\n",
        "\n",
        "        # Loop fold cross validation\n",
        "        fold_idx = 0\n",
        "        for train_idx, val_idx in mskf.split(X, Y):\n",
        "            fold_idx += 1\n",
        "            print(\"\\n\" + \"-\"*160)\n",
        "            print(f\"--- Fold {fold_idx}/{num_folds} (cfg {cfg_id}) ---\")\n",
        "            start_fold = time.time()\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
        "\n",
        "            cache_key = (fold_idx, npmi_th)\n",
        "            if cache_key in graph_cache:\n",
        "                A_train, vocab_train_words, node_embs_for_graph = graph_cache[cache_key]\n",
        "                print(\" Loaded adjacency & node-embeds from cache.\")\n",
        "            else:\n",
        "                # Graph\n",
        "                A_train, vocab_train = build_npmi_adjacency(X_train, max_vocab=max_vocab, window=window, min_cooc=min_cooc, npmi_th=npmi_th)\n",
        "                if A_train is None:\n",
        "                    V = len(vocab_words_full)\n",
        "                    A_train = sp.eye(V, format='csr')\n",
        "                    vocab_train_words = vocab_words_full\n",
        "                else:\n",
        "                    vocab_train_words = sorted(vocab_train.keys(), key=lambda w: vocab_train[w])\n",
        "                # Node embedding untuk graph\n",
        "                node_embs_for_graph = build_node_embeddings_from_bert(vocab_train_words, bert_name_node_emb, device=DEVICE, batch_size=128, shared_model=node_model_shared, shared_tokenizer=node_tokenizer_shared)\n",
        "                if node_embs_for_graph.shape[0] == 0:\n",
        "                    node_embs_for_graph = np.random.normal(size=(len(vocab_train_words), node_model_shared.config.hidden_size)).astype(np.float32)\n",
        "                graph_cache[cache_key] = (A_train, vocab_train_words, node_embs_for_graph)\n",
        "                print(\" Cache saved for (fold, npmi_th) =\", cache_key)\n",
        "\n",
        "            node_embs_for_graph_t = torch.tensor(node_embs_for_graph, dtype=torch.float32, device=DEVICE)\n",
        "            A_train_torch = sparse_scipy_to_torch_sparse(A_train, device=DEVICE)\n",
        "\n",
        "            # graph feature per dokumen (BoW-weighted node embedding)\n",
        "            vect = CountVectorizer(vocabulary=vocab_train_words)\n",
        "            X_train_bow = vect.fit_transform(X_train)\n",
        "            X_val_bow = vect.transform(X_val)\n",
        "            graph_feat_train = compute_sample_graph_feats(X_train_bow, node_embs_for_graph)\n",
        "            graph_feat_val = compute_sample_graph_feats(X_val_bow, node_embs_for_graph)\n",
        "            graph_feat_train_scaled = graph_feat_train * graph_scale\n",
        "            graph_feat_val_scaled = graph_feat_val * graph_scale\n",
        "\n",
        "            # Siapkan tokenizer utama & DataLoader\n",
        "            tokenizer_main = get_tokenizer(bert_name_main, max_len)\n",
        "            bert_main_model = AutoModel.from_pretrained(bert_name_main).to(DEVICE)\n",
        "            gcn_instance = SimpleGCN(in_dim=node_embs_for_graph.shape[1], hidden_dim=gcn_hidden, n_layers=gcn_layers)\n",
        "\n",
        "            # Dataset + DataLoader per fold\n",
        "            train_ds = PersonalityDataset(X_train, Y_train, tokenizer_main, max_len=max_len, graph_feats=graph_feat_train_scaled)\n",
        "            val_ds   = PersonalityDataset(X_val, Y_val, tokenizer_main, max_len=max_len, graph_feats=graph_feat_val_scaled)\n",
        "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "            val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "            # model VGCN-BERT dan komponen training\n",
        "            model = VGCN_BERT_Model(\n",
        "                bert_model=bert_main_model,\n",
        "                gcn=gcn_instance,\n",
        "                hidden_dim=gcn_hidden,\n",
        "                num_classes=5,\n",
        "                dropout=dropout\n",
        "            ).to(DEVICE)\n",
        "            # loss:\n",
        "            pos_weight = compute_pos_weight(Y_train)\n",
        "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "            # optimizer:\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "            scaler = GradScaler()\n",
        "            total_steps = max(1, len(train_loader) * epochs)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, total_steps)) # lr scheduler\n",
        "\n",
        "            best_val = 0.0; no_imp = 0; best_state = None; best_thresholds = [0.5]*5\n",
        "\n",
        "            # Training loop + Early stopping\n",
        "            for epoch in range(epochs):\n",
        "                t0 = time.time()\n",
        "                # training\n",
        "                model.train()\n",
        "                train_losses = []\n",
        "                for batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    labels = batch.pop('labels').to(DEVICE)\n",
        "                    graph_feats_batch = batch.pop('graph').to(DEVICE)\n",
        "                    input_ids = batch['input_ids'].to(DEVICE); attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                    with autocast():\n",
        "                        out = model(input_ids=input_ids, attention_mask=attention_mask, graph_node_emb=node_embs_for_graph_t, A=A_train_torch, graph_feat=graph_feats_batch)\n",
        "                        logits = out\n",
        "                        loss = criterion(logits, labels)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    scaler.step(optimizer); scaler.update()\n",
        "                    scheduler.step()\n",
        "                    train_losses.append(loss.item())\n",
        "\n",
        "                # validation\n",
        "                model.eval()\n",
        "                preds = []; trues = []\n",
        "                with torch.no_grad():\n",
        "                    for vb in val_loader:\n",
        "                        v_labels = vb['labels'].numpy()\n",
        "                        v_graph_feats = vb['graph'].to(DEVICE)\n",
        "                        v_input_ids = vb['input_ids'].to(DEVICE); v_attn = vb['attention_mask'].to(DEVICE)\n",
        "                        out = model(input_ids=v_input_ids, attention_mask=v_attn, graph_node_emb=node_embs_for_graph_t, A=A_train_torch, graph_feat=v_graph_feats)\n",
        "                        probs = torch.sigmoid(out).cpu().numpy()\n",
        "                        preds.append(probs); trues.append(v_labels)\n",
        "                preds = np.vstack(preds); trues = np.vstack(trues)\n",
        "                # threshold tuning per epoch:\n",
        "                thrs = tune_thresholds(trues, preds, low=0.2, high=0.7, step=0.01)\n",
        "                pred_labels = (preds >= thrs).astype(int)\n",
        "                val_macro = f1_score(trues, pred_labels, average='macro', zero_division=0)\n",
        "                val_weighted = f1_score(trues, pred_labels, average='weighted', zero_division=0)\n",
        "                epoch_time = time.time() - t0\n",
        "                improved_marker = \"\"\n",
        "\n",
        "                if val_macro > best_val + 1e-5:\n",
        "                    best_val = val_macro\n",
        "                    best_state = {'state_dict': model.state_dict(), 'thresholds': thrs}\n",
        "                    no_imp = 0\n",
        "                    improved_marker = \"improved \\u2713\"\n",
        "                else:\n",
        "                    no_imp += 1\n",
        "\n",
        "                print(f\"Epoch {epoch+1} | train_loss {np.mean(train_losses):.4f} | val_macro {val_macro:.4f} | val_weighted {val_weighted:.4f} | time {epoch_time:.1f}s {improved_marker}\")\n",
        "\n",
        "                if no_imp >= patience:\n",
        "                    print(\"Early stopping (patience=\",patience,\") at epoch\", epoch+1)\n",
        "                    break\n",
        "\n",
        "            fold_time = time.time() - start_fold\n",
        "            print(f\"Best val (fold {fold_idx}) macro F1: {best_val:.4f} | time {fold_time:.1f}s\")\n",
        "\n",
        "            model.load_state_dict(best_state['state_dict'])\n",
        "            model.eval()\n",
        "            preds = []\n",
        "            with torch.no_grad():\n",
        "                for vb in val_loader:\n",
        "                    v_graph_feats = vb['graph'].to(DEVICE)\n",
        "                    v_input_ids = vb['input_ids'].to(DEVICE); v_attn = vb['attention_mask'].to(DEVICE)\n",
        "                    out = model(input_ids=v_input_ids, attention_mask=v_attn, graph_node_emb=node_embs_for_graph_t, A=A_train_torch, graph_feat=v_graph_feats)\n",
        "                    probs = torch.sigmoid(out).cpu().numpy()\n",
        "                    preds.append(probs)\n",
        "            preds = np.vstack(preds)\n",
        "\n",
        "            oof_preds[val_idx] = preds\n",
        "            oof_trues[val_idx] = Y_val\n",
        "\n",
        "            fold_scores.append(best_val)\n",
        "            fold_infos.append({\"fold\": fold_idx, \"vocab_size\": len(vocab_train_words), \"best_val\": best_val})\n",
        "\n",
        "            del model; torch.cuda.empty_cache()\n",
        "\n",
        "        # Evaluasi agregat OOF untuk konfigurasi itu\n",
        "        oof_preds_clipped = np.clip(oof_preds, 1e-6, 1-1e-6)\n",
        "        best_thrs_cfg = tune_thresholds(oof_trues, oof_preds_clipped, low=0.2, high=0.7, step=0.01)\n",
        "        oof_pred_labels_cfg = (oof_preds_clipped >= best_thrs_cfg).astype(int)\n",
        "        macro_f1_cfg = f1_score(oof_trues, oof_pred_labels_cfg, average='macro', zero_division=0)\n",
        "        weighted_f1_cfg = f1_score(oof_trues, oof_pred_labels_cfg, average='weighted', zero_division=0)\n",
        "        print(\"\\nCONFIG RESULT:\", cfg)\n",
        "        print(\"Mean fold val macro:\", np.mean(fold_scores), \"std:\", np.std(fold_scores))\n",
        "        print(\"Aggregated OOF Macro F1:\", macro_f1_cfg, \"Weighted F1:\", weighted_f1_cfg)\n",
        "\n",
        "        # save\n",
        "        cfg_out = {\n",
        "            \"config\": cfg,\n",
        "            \"fold_scores\": fold_scores,\n",
        "            \"fold_infos\": fold_infos,\n",
        "            \"mean_fold\": float(np.mean(fold_scores)),\n",
        "            \"std_fold\": float(np.std(fold_scores)),\n",
        "            \"oof_macro\": float(macro_f1_cfg),\n",
        "            \"oof_weighted\": float(weighted_f1_cfg),\n",
        "            \"thresholds\": [float(t) for t in best_thrs_cfg]\n",
        "        }\n",
        "        np.save(f\"vgcn_cfg_{cfg_id}_oof_preds.npy\", oof_preds_clipped)\n",
        "        np.save(f\"vgcn_cfg_{cfg_id}_oof_trues.npy\", oof_trues)\n",
        "        results.append(cfg_out)\n",
        "        with open(f\"vgcn_cfg_{cfg_id}_summary.txt\",\"w\") as f:\n",
        "            f.write(str(cfg_out))\n",
        "        print(\"Saved OOF & summary for config\", cfg_id)\n",
        "\n",
        "    # choose best\n",
        "    best = max(results, key=lambda r: r['oof_macro'])\n",
        "    best_idx = results.index(best) + 1\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEST CONFIG index:\", best_idx, \"config:\", best['config'])\n",
        "    print(\"Best OOF Macro F1:\", best['oof_macro'], \"Weighted:\", best['oof_weighted'])\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\"results\": results, \"best_idx\": best_idx, \"best\": best, \"best_oof_preds_file\": f\"vgcn_cfg_{best_idx}_oof_preds.npy\", \"best_oof_trues_file\": f\"vgcn_cfg_{best_idx}_oof_trues.npy\"}"
      ],
      "metadata": {
        "id": "AP8DLIQmuMPA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.103044Z",
          "iopub.execute_input": "2026-01-04T10:11:11.103551Z",
          "iopub.status.idle": "2026-01-04T10:11:11.133799Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.103524Z",
          "shell.execute_reply": "2026-01-04T10:11:11.133286Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 12) Run grid search + training\n",
        "# ----------------------------\n",
        "out = run_grid_search_and_train(df,\n",
        "                               grid=GRID,\n",
        "                               bert_name_main=CONFIG[\"BERT_MAIN\"],\n",
        "                               bert_name_node_emb=CONFIG[\"BERT_FOR_NODE_EMB\"],\n",
        "                               max_vocab=RUN[\"max_vocab\"],\n",
        "                               window=CONFIG[\"WINDOW\"],\n",
        "                               min_cooc=CONFIG[\"MIN_COOC\"],\n",
        "                               num_folds=RUN[\"num_folds\"],\n",
        "                               epochs=RUN[\"epochs\"],\n",
        "                               batch_size=RUN[\"batch_size\"],\n",
        "                               patience=CONFIG[\"PATIENCE\"])"
      ],
      "metadata": {
        "id": "7TfTXIH4uUZK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T10:11:11.134676Z",
          "iopub.execute_input": "2026-01-04T10:11:11.135224Z",
          "iopub.status.idle": "2026-01-04T16:43:22.057325Z",
          "shell.execute_reply.started": "2026-01-04T10:11:11.135202Z",
          "shell.execute_reply": "2026-01-04T16:43:22.056521Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 13) Load best OOF, final error analysis & confusion plots\n",
        "# ----------------------------\n",
        "best_idx = out['best_idx']\n",
        "oof_preds = np.load(f\"vgcn_cfg_{best_idx}_oof_preds.npy\")\n",
        "oof_trues = np.load(f\"vgcn_cfg_{best_idx}_oof_trues.npy\")\n",
        "best_thresholds = out['best']['thresholds']\n",
        "oof_pred_labels = (oof_preds >= best_thresholds).astype(int)\n",
        "\n",
        "print(\"\\nFinal best config:\", out['best']['config'])\n",
        "print(\"Final thresholds:\", best_thresholds)"
      ],
      "metadata": {
        "id": "BF-_HlTmLdfn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T16:43:22.058318Z",
          "iopub.execute_input": "2026-01-04T16:43:22.059032Z",
          "iopub.status.idle": "2026-01-04T16:43:22.065448Z",
          "shell.execute_reply.started": "2026-01-04T16:43:22.059005Z",
          "shell.execute_reply": "2026-01-04T16:43:22.064865Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 14) Evaluation metrics per trait (best config)\n",
        "# ----------------------------\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "y_true = oof_trues\n",
        "y_pred = oof_pred_labels\n",
        "trait_names = ['O','C','E','A','N']\n",
        "\n",
        "metrics = []\n",
        "accs = []\n",
        "\n",
        "for i, t in enumerate(trait_names):\n",
        "    y_t = y_true[:, i]\n",
        "    y_p = y_pred[:, i]\n",
        "\n",
        "    prec = precision_score(y_t, y_p, zero_division=0)\n",
        "    rec  = recall_score(y_t, y_p, zero_division=0)\n",
        "    f1   = f1_score(y_t, y_p, zero_division=0)\n",
        "    acc  = accuracy_score(y_t, y_p)\n",
        "\n",
        "    metrics.append([t, prec, rec, f1, acc])\n",
        "    accs.append(acc)\n",
        "\n",
        "metrics_table = pd.DataFrame(\n",
        "    metrics,\n",
        "    columns=['Trait', 'Precision', 'Recall', 'F1', 'Accuracy']\n",
        ")\n",
        "\n",
        "macro_accuracy = np.mean(accs)\n",
        "\n",
        "supports = [len(y_true[:, i]) for i in range(y_true.shape[1])]\n",
        "weighted_accuracy = np.average(accs, weights=supports)\n",
        "\n",
        "macro_avg = [\n",
        "    \"Macro Average\",\n",
        "    precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "    recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "    f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "    macro_accuracy\n",
        "]\n",
        "weighted_avg = [\n",
        "    \"Weighted Average\",\n",
        "    precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "    recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "    f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "    weighted_accuracy\n",
        "]\n",
        "\n",
        "metrics_table = pd.concat(\n",
        "    [metrics_table, pd.DataFrame([macro_avg, weighted_avg], columns=metrics_table.columns)],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "print(\"Evaluation Metrics per Trait (Best Config):\")\n",
        "display(metrics_table)"
      ],
      "metadata": {
        "id": "obqNah45XmiD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T16:43:22.068565Z",
          "iopub.execute_input": "2026-01-04T16:43:22.068927Z",
          "iopub.status.idle": "2026-01-04T16:43:22.135183Z",
          "shell.execute_reply.started": "2026-01-04T16:43:22.068904Z",
          "shell.execute_reply": "2026-01-04T16:43:22.134584Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 15) Plot F1 vs Threshold (per trait)\n",
        "# ----------------------------\n",
        "plot_f1_vs_thresholds(oof_trues, oof_preds)\n",
        "\n",
        "print(\"\\nPer-trait F1 before (0.5) and after (tuned):\")\n",
        "for i, t in enumerate(['O','C','E','A','N']):\n",
        "    f_before = f1_score(oof_trues[:,i], (oof_preds[:,i] >= 0.5).astype(int), zero_division=0)\n",
        "    f_after = f1_score(oof_trues[:,i], (oof_preds[:,i] >= best_thresholds[i]).astype(int), zero_division=0)\n",
        "    print(f\"{t}: before={f_before:.4f} → after={f_after:.4f}\")"
      ],
      "metadata": {
        "id": "kKy2jQbdhME6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T16:43:22.136042Z",
          "iopub.execute_input": "2026-01-04T16:43:22.136282Z",
          "iopub.status.idle": "2026-01-04T16:43:23.131999Z",
          "shell.execute_reply.started": "2026-01-04T16:43:22.136260Z",
          "shell.execute_reply": "2026-01-04T16:43:23.131389Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 16) Error Analysis & Confusion Matrix (all traits)\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
        "\n",
        "def error_analysis_all_traits(y_true, y_pred_labels, thresholds=None, trait_names=('O','C','E','A','N')):\n",
        "    print(\"Error analysis per trait:\\n\")\n",
        "\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        y_t = y_true[:, i].astype(int)\n",
        "        y_p = y_pred_labels[:, i].astype(int)\n",
        "\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "            y_t, y_p, average='binary', zero_division=0\n",
        "        )\n",
        "\n",
        "        cm = multilabel_confusion_matrix(y_t, y_p)[0]\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        total = tn + fp + fn + tp\n",
        "        acc = (tp + tn) / total if total > 0 else 0.0\n",
        "        support_pos = int(y_t.sum())\n",
        "        thr = thresholds[i] if thresholds is not None and len(thresholds) > i else None\n",
        "\n",
        "        print(f\"Trait {trait}:\")\n",
        "        if thr is not None:\n",
        "            print(f\" Threshold={thr:.3f}\")\n",
        "        print(f\" Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, Accuracy={acc:.3f}, Support(pos)={support_pos}\")\n",
        "\n",
        "        print(f\" Confusion matrix (tp, fn, fp, tn) = {(int(tp), int(fn), int(fp), int(tn))}\")\n",
        "        print(f\"  TP={int(tp)}, TN={int(tn)}, FP={int(fp)}, FN={int(fn)}\\n\")\n",
        "\n",
        "# Error analysis\n",
        "error_analysis_all_traits(\n",
        "    y_true=oof_trues,\n",
        "    y_pred_labels=oof_pred_labels,\n",
        "    thresholds=best_thresholds,\n",
        "    trait_names=('O','C','E','A','N')\n",
        ")\n",
        "\n",
        "# Plot gabungan confusion matrix untuk semua traits\n",
        "cms = multilabel_confusion_matrix(oof_trues, oof_pred_labels)\n",
        "trait_names = ['O','C','E','A','N']\n",
        "n = len(trait_names)\n",
        "\n",
        "fig, axes = plt.subplots(1, n, figsize=(4*n, 4))\n",
        "if n == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i in range(n):\n",
        "    cm = cms[i]  # [[TN, FP],[FN, TP]]\n",
        "    ax = axes[i]\n",
        "    ax.imshow(cm, interpolation='nearest')\n",
        "    ax.set_title(trait_names[i])\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels([\"Neg(0)\", \"Pos(1)\"])\n",
        "    ax.set_yticklabels([\"Neg(0)\", \"Pos(1)\"])\n",
        "\n",
        "    for (r, c), v in np.ndenumerate(cm):\n",
        "        ax.text(c, r, int(v), ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fl8RBJqAANxR",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T16:43:23.132907Z",
          "iopub.execute_input": "2026-01-04T16:43:23.133164Z",
          "iopub.status.idle": "2026-01-04T16:43:24.152213Z",
          "shell.execute_reply.started": "2026-01-04T16:43:23.133140Z",
          "shell.execute_reply": "2026-01-04T16:43:24.151517Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 17) Threshold Range Evaluation\n",
        "# ----------------------------\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "def evaluate_thresholds_all_metrics(oof_trues, oof_preds, low, high, step):\n",
        "    threshs = tune_thresholds(oof_trues, oof_preds, low=low, high=high, step=step)\n",
        "    pred_labels = (oof_preds >= threshs).astype(int)\n",
        "\n",
        "    accs  = [accuracy_score(oof_trues[:, i], pred_labels[:, i]) for i in range(oof_trues.shape[1])]\n",
        "    precisions = [precision_score(oof_trues[:, i], pred_labels[:, i], zero_division=0) for i in range(oof_trues.shape[1])]\n",
        "    recalls    = [recall_score(   oof_trues[:, i], pred_labels[:, i], zero_division=0) for i in range(oof_trues.shape[1])]\n",
        "    f1s        = [f1_score(       oof_trues[:, i], pred_labels[:, i], zero_division=0) for i in range(oof_trues.shape[1])]\n",
        "\n",
        "    cms = multilabel_confusion_matrix(oof_trues, pred_labels)\n",
        "\n",
        "    metrics = {\n",
        "        \"thresholds\": threshs,\n",
        "        \"accuracy_per_label\": accs,\n",
        "        \"precision_per_label\": precisions,\n",
        "        \"recall_per_label\": recalls,\n",
        "        \"f1_per_label\": f1s,\n",
        "        \"avg_acc\": np.mean(accs),\n",
        "        \"avg_prec\": np.mean(precisions),\n",
        "        \"avg_rec\": np.mean(recalls),\n",
        "        \"avg_f1\": np.mean(f1s),\n",
        "        \"confusion_matrices\": cms,\n",
        "        \"pred_labels\": pred_labels\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Threshold ranges\n",
        "threshold_ranges = [\n",
        "    (0.2, 0.7, 0.01),\n",
        "    (0.3, 0.8, 0.01),\n",
        "    (0.4, 0.9, 0.01),\n",
        "    (0.5, 1.0, 0.01),\n",
        "]\n",
        "\n",
        "trait_names = ['O','C','E','A','N']\n",
        "\n",
        "best_avg_f1_overall = -1\n",
        "best_row = None\n",
        "\n",
        "\n",
        "for low, high, step in threshold_ranges:\n",
        "    print(f\"\\n=== Threshold Range {low}-{high} (step {step}) ===\")\n",
        "\n",
        "    metrics = evaluate_thresholds_all_metrics(oof_trues, oof_preds, low, high, step)\n",
        "\n",
        "    rows = []\n",
        "    supports = []\n",
        "\n",
        "    for i, t in enumerate(trait_names):\n",
        "        y_true_t = oof_trues[:, i]\n",
        "        y_pred_t = metrics[\"pred_labels\"][:, i]\n",
        "        support = y_true_t.sum()\n",
        "        supports.append(support)\n",
        "\n",
        "        rows.append([\n",
        "            t,\n",
        "            metrics[\"precision_per_label\"][i],\n",
        "            metrics[\"recall_per_label\"][i],\n",
        "            metrics[\"f1_per_label\"][i],\n",
        "            metrics[\"accuracy_per_label\"][i]\n",
        "        ])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"Trait\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"])\n",
        "\n",
        "    macro_row = [\n",
        "        \"Macro Average\",\n",
        "        df[\"Precision\"].mean(),\n",
        "        df[\"Recall\"].mean(),\n",
        "        df[\"F1\"].mean(),\n",
        "        df[\"Accuracy\"].mean()\n",
        "    ]\n",
        "\n",
        "    weights = np.array(supports) / np.sum(supports)\n",
        "    weighted_row = [\n",
        "        \"Weighted Average\",\n",
        "        np.average(df[\"Precision\"], weights=weights),\n",
        "        np.average(df[\"Recall\"], weights=weights),\n",
        "        np.average(df[\"F1\"], weights=weights),\n",
        "        np.average(df[\"Accuracy\"], weights=weights)\n",
        "    ]\n",
        "\n",
        "    df = pd.concat(\n",
        "        [df, pd.DataFrame([macro_row, weighted_row], columns=df.columns)],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    print(\"Thresholds per trait:\", dict(zip(trait_names, metrics[\"thresholds\"])))\n",
        "    display(df)\n",
        "\n",
        "    # Best range check\n",
        "    if metrics[\"avg_f1\"] > best_avg_f1_overall:\n",
        "        best_avg_f1_overall = metrics[\"avg_f1\"]\n",
        "        best_row = {\n",
        "            \"range\": f\"{low:.2f}-{high:.2f}@{step}\",\n",
        "            \"thresholds\": dict(zip(trait_names, metrics[\"thresholds\"])),\n",
        "            \"avg_acc\": metrics[\"avg_acc\"],\n",
        "            \"avg_prec\": metrics[\"avg_prec\"],\n",
        "            \"avg_rec\": metrics[\"avg_rec\"],\n",
        "            \"avg_f1\": metrics[\"avg_f1\"]\n",
        "        }\n",
        "\n",
        "    # Confusion matrix\n",
        "    fig, axes = plt.subplots(1, len(trait_names), figsize=(4 * len(trait_names), 4))\n",
        "    for i, ax in enumerate(axes):\n",
        "        sns.heatmap(metrics[\"confusion_matrices\"][i], annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
        "        ax.set_title(f\"{trait_names[i]} | thr={metrics['thresholds'][i]:.2f}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Best Threshold Range Based on Avg F1 ---\")\n",
        "print(f\"Range        : {best_row['range']}\")\n",
        "print(f\"Avg F1       : {best_row['avg_f1']:.4f}\")\n",
        "print(f\"Avg Accuracy : {best_row['avg_acc']:.4f}\")\n",
        "print(f\"Avg Precision: {best_row['avg_prec']:.4f}\")\n",
        "print(f\"Avg Recall   : {best_row['avg_rec']:.4f}\")\n",
        "\n",
        "print(\"Best Thresholds per trait:\")\n",
        "for t, v in best_row[\"thresholds\"].items():\n",
        "    print(f\"  {t}: {v:.2f}\")"
      ],
      "metadata": {
        "id": "Ci9O20SaeDqH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-04T16:43:24.153153Z",
          "iopub.execute_input": "2026-01-04T16:43:24.153402Z",
          "iopub.status.idle": "2026-01-04T16:43:30.437967Z",
          "shell.execute_reply.started": "2026-01-04T16:43:24.153380Z",
          "shell.execute_reply": "2026-01-04T16:43:30.437168Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}